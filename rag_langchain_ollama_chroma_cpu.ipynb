{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe",
   "metadata": {
    "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe"
   },
   "source": [
    "# Locally using Ollama Rest end point\n",
    "\n",
    "Make sure to have docker installed locally\n",
    "Pull Ollama image using and serve using CPU only option\n",
    "\n",
    "docker pull ollama/ollama\n",
    "\n",
    "docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "CURL command to test if your local setup works\n",
    "\n",
    "curl --location 'http://localhost:11434/api/generate' \\\n",
    "--header 'Content-Type: application/json' \\\n",
    "--data '{\n",
    "  \"model\": \"llama3.2:1b\",\n",
    "  \"prompt\": \"Translate the following English text to French: '\\''Hello, how are you?'\\''\",\"max_tokens\": 10\n",
    "}'\n",
    "\n",
    "We will do the same from Python now (version used 3.12.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "_PVwk-KPv22G",
   "metadata": {
    "id": "_PVwk-KPv22G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of \"Hello, how are you?\" from English to French is:\n",
      "\n",
      "\"Bonjour, comment vas-tu ?\"\n",
      "\n",
      "Here's a breakdown of each word:\n",
      "\n",
      "- Bonjour (good day)\n",
      "- comment (how, in this case, meaning \"in what way\", but also literally \"what\")\n",
      "- vas-tu (you)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "# Set environment variable for Ollama model URL\n",
    "os.environ['OLLAMA_API_URL'] = 'http://localhost:11434/api/generate'\n",
    "\n",
    "# Function to query the Ollama model\n",
    "def query_ollama_model(prompt):    \n",
    "    url = os.environ['OLLAMA_API_URL']    \n",
    "    payload = {\n",
    "    \"model\": \"llama3.2:1b\",\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 60\n",
    "    }\n",
    "    # Convert the payload to a JSON string\n",
    "    payload_json = json.dumps(payload)\n",
    "\n",
    "    # Set the headers to specify JSON content\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Send the POST request\n",
    "    response = requests.post(url, data=payload_json, headers=headers, stream=True)\n",
    "    # Check the response\n",
    "    if response.status_code == 200:\n",
    "        full_response = \"\"\n",
    "        # Process each line in the streaming response\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                # Decode the line and parse it as JSON\n",
    "                json_response = json.loads(line.decode('utf-8'))\n",
    "                if 'response' in json_response:\n",
    "                    full_response += json_response['response']\n",
    "                if json_response.get('done', False):\n",
    "                    break\n",
    "        return full_response\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        response.raise_for_status()\n",
    "        \n",
    "# Example usage\n",
    "prompt = \"Translate the following English text to French: 'Hello, how are you?'\"\n",
    "response_text = query_ollama_model(prompt)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf182b76",
   "metadata": {},
   "source": [
    "# Rag Locally using Ollama\n",
    "\n",
    "Prerequiste for Chroma to work locally :\n",
    "\n",
    "Download: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "\n",
    "Install \"Desktop development with C++\"\n",
    "\n",
    "For Ollama to work without Docker, install the package from :\n",
    "\n",
    "https://github.com/ollama/ollama?tab=readme-ov-file\n",
    "\n",
    "Then go to CMD, type\n",
    "\n",
    "ollama pull llama3.2:1b\n",
    "\n",
    "You also need to pull \"nomic-embed-text\" in similar manner.\n",
    "This is for generating your embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a8cf3d",
   "metadata": {},
   "source": [
    "Now we know that our local model works. And by giving a prompt without any external context, i was able to get back response from my model. And able to print the response. Next step would be to demonstrate the same with a RAG pipeline, where we would add a context along with the payload to get back a response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21f756b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\dewan.hussain\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script watchfiles.exe is installed in 'C:\\Users\\dewan.hussain\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script uvicorn.exe is installed in 'C:\\Users\\dewan.hussain\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script humanfriendly.exe is installed in 'C:\\Users\\dewan.hussain\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\dewan.hussain\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pyproject-build.exe is installed in 'C:\\Users\\dewan.hussain\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script typer.exe is installed in 'C:\\Users\\dewan.hussain\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script fastapi.exe is installed in 'C:\\Users\\dewan.hussain\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script coloredlogs.exe is installed in 'C:\\Users\\dewan.hussain\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts opentelemetry-bootstrap.exe and opentelemetry-instrument.exe are installed in 'C:\\Users\\dewan.hussain\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script onnxruntime_test.exe is installed in 'C:\\Users\\dewan.hussain\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script chroma.exe is installed in 'C:\\Users\\dewan.hussain\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Document loading, retrieval methods and text splitting\n",
    "%pip install -qU langchain langchain_community\n",
    "\n",
    "# Local vector store via Chroma\n",
    "%pip install -qU langchain_chroma\n",
    "\n",
    "# Local inference and embeddings via Ollama\n",
    "%pip install -qU langchain_ollama\n",
    "\n",
    "# Web Loader\n",
    "%pip install -qU beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98070313-0c2f-4ba6-ae3e-79e2418ce4df",
   "metadata": {
    "id": "98070313-0c2f-4ba6-ae3e-79e2418ce4df"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dewan.hussain\\AppData\\Roaming\\Python\\Python312\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables FIRST, before any other imports\n",
    "load_dotenv()\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"llanchain-api-key\"  # Add this line\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"your-project-name\"  # Add this line\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"  # Add this line\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain import hub\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\"https://www.cherryhillfreeclinic.org/care-services/\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Check if docs is empty and handle it\n",
    "if not docs:\n",
    "    print(\"No documents loaded. Check the web paths or bs_kwargs.\")\n",
    "else:\n",
    "    # Split\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Check if splits is empty and handle it\n",
    "    if not splits:\n",
    "        print(\"No splits generated. Check the text splitter configuration.\")\n",
    "    else:\n",
    "        #Local Embeddings for Ollama\n",
    "        local_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "        \n",
    "        # Embed\n",
    "        vectorstore = Chroma.from_documents(documents=splits, embedding=local_embeddings)\n",
    "\n",
    "        retriever = vectorstore.as_retriever()\n",
    "\n",
    "        #### RETRIEVAL and GENERATION ####\n",
    "\n",
    "        # Prompt\n",
    "        prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "        # LLM\n",
    "        #llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "        llm = ChatOllama(\n",
    "            model=\"llama3.2:1b\",\n",
    "        )\n",
    "\n",
    "        # Post-processing\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "        # Chain\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        # Question\n",
    "        rag_chain.invoke(\"any treatments for ADHD available?\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
